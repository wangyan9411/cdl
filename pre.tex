\documentclass{template}

\begin{document}

\title{entity-aware Collaborative Filtering Recommendation}

\date{}

\author{Yan Wang}
\maketitle

\section{Problem definition}
Social entityging Systems is a new and popular family of Web applications. entities can represent additional and personalized information about resources, which if properly exploited, can eventually boost the performance of resource recommender system.

Formally, a folksonomy is a tuple $F := (U,T,R,Y)$ where
Y is a ternary relation between them, $Y \subseteq U\times T\times R$, whose elements are called entity assignments. Users are typically described by their user ID, and entities may be arbitrary strings. The resources can be movies in MovieLens, and in Aminer, resources are academic activities. The set of triples in $Y$ can be represented as third-order tensors $A = (a_{u,t,r}) \in R^{|U|\times |T|\times |R|}$.

We divide the recommending process into two steps, mining the user profile, and then, recommending based on user profile and resource content.

To construct the user profile, the tensor is reduced to a lower dimensional space, known as user-entity projection matrix, $(\pi_{UT}Y) \in \left\{ 0, 1 \right\}^{|U|\times |Y|}$. Thus, constructing user profile is the problem of making recommendations about those entities that have been assigned by at least one user in the system, and also the new entities that never assigned.

\section{Model}
\subsection{Model}
To incorporate the semantic information revealed by word vectors of entity strings, our algorithm combines word embedding model and traditional collaborative filtering based on latent factor model.

\subsection{Entity Embedding}
In the embedding model, we use the entity to predict the surrounding context. In the entity embedding, we consider the problem of learning representations for entities based on their associations with its descriptive unstructured documents. Given entities and their corresponding description, we define $f$ to be the mapping between context and entity representations. 
\begin{equation}
\begin{aligned}
f = tanh(W\cdot e + b)
\end{aligned}
\end{equation}

we apply a variant of Noise-Contrastive Estimation (NCE) where we sample negative instances from a noise distribution with replacement. We use the uniform distribution over context as noise distribution. Define
\begin{equation}
\begin{aligned}
P(S|c_i, f_{e_i}) = \sigma(c_i \cdot f_{e_i})
\end{aligned}
\end{equation}
as the similarity of two representations in latent vector space, where
\begin{equation}
\sigma(t) = \frac{1}{1 + e^{-t}}
\end{equation}

We then approximate the probability of context word given a entity by randomly sampling z contrastive examples.The objective of the model is to maximize the conditional log-likelihood of all context.
\begin{equation}
\begin{aligned}
L &= \sum_{e\in E} \sum_{c_i\in context} log(p(c_i|f_e)) \\
&= \sum_{e\in E} \sum_{c_i\in context}(P(S|c_i, f_{e}) + \sum_{k=1}^z log(P(S|\tilde c_k, f_{e})))
\end{aligned}
\end{equation}


\subsection{Recommendation by Matrix Factorization}
According to the routine of SVD model, a rating is predicted by the rule:
\begin{equation}
\hat{r}_{i,j} = u_i^T \cdot v_j
\end{equation}

To use matrix factorization, we must compute the latent representations of the users and items given an observed matrix of ratings.
The common approach is to minimize the regularized squared error loss with respect to U and V,
\begin{equation}
\sum_{i,j} (r_{ij} - u_i \cdot v_j )^2 + \lambda_v||v_j|| + \lambda_u||u_i||
\end{equation}
where $\lambda_u$ and  $\lambda_v$ are regularization parameters.

\subsection{Combination}
The combination is very straightforward, entity embedding may capture the relations or the similarities between different entities. These vectors strive to map entities into an k-dimensional latent factor space where they are measured against various aspects that are revealed automatically by learning from corpus data. So we could use it as the item vector in Matrix Factorization method for recommendation.

According to the routine of SVD model, a rating is predicted by the rule:
\begin{equation}
\hat{r}_{u_i} = \mu+ b_u+ b_i+ q_i^T\cdot p_u
\end{equation}

Accordingly, each item $i$ is associated with a vector $q_i \in R^f$ , and each user $u$ is associated with a vector $p_u \in R^f$ . For a given entity $i$, the elements of $q_i$ measure the extent to which the entity possesses those factors, while the elements of $p_u$ measure the extent of interest the user has in entities that are high on the corresponding factors. Here the elements of $q_i$ could be represented by corresponding word vector of the entity item, because the word vector $v_i$ can be interpreted in another way as the vector of latent factors of the word $i$.
The rule is rewritten as:
\begin{equation}
\hat{r}_{u_i} = v_i^T\cdot p_u
\end{equation}
Implicit preference by users for entity $i$ leads us to adjust the $v_i$ by $\theta_i$ to capture divergence between entities of similar meaning.
So the exact model is as follows:
\begin{equation}
\hat{r}_{u_i} = (v_i^T + \theta_i^T) \cdot p_u
\end{equation}

Again, the $\theta_i$ captures the interaction bias between users and the entity $i$ besides the word vector $v_i$. It's of the intuition that, similar entities of very close representation by word embedding, could attract different kinds of user groups. 


In order to learn the model parameters $(b_u,b_i, p_u, v_i \text{and}  \theta_i)$. We need to jointly minimize the loss function in Eq. 4 and Eq. 6.
\begin{equation}
\begin{aligned}
L &= L_{EMD} + L_{MF} \\
&= \sum_{i,j} (r_{ij} - u_i \cdot v_j )^2 + \lambda_v||v_j|| + \lambda_u||u_i|| 
\\ &+ \sum_{e\in E} \sum_{c_i\in context} log(p(c_i|f_e))
\end{aligned}
\end{equation}


\subsection{Parameter Estimation}
A stochastic gradient descent optimization is developed to estimate the parameter. The algorithm loops through all ratings in the training data. For each given rating $r_{ui}$, a prediction $\hat{r_{ui}}$ is calculated, and the associated prediction error $e_{ui} = r_{ui} - \hat{r_{ui}}$ is computed. For a given training case $r_{ui}$, we modify the parameters by moving in the opposite direction of the gradient, yielding:

\begin{equation}
\begin{aligned}
& p_u \leftarrow p_u + \gamma \cdot (e_{ui}\cdot q_i - \lambda_1 \cdot p_u) \\
& b_i \leftarrow b_i + \gamma \cdot (e_{ui}\cdot p_u - \lambda_1 \cdot b_i) \\
& W \leftarrow W + \gamma \cdot (e_{ui}\cdot p_u\cdot x_i^T - \lambda_3 \cdot W)
\end{aligned}
\end{equation}


\section{Data Set}
\textbf{Last.fm.} 20000 entities applied to artists by 1000 users. Last.fm is a popular music system that provides personalized radio stations for its users. The data is gathered during August 2016 through their web services API. 

\textbf{MovieLens.}

\textbf{Aminer.}
Most convenient is high quality explicit feedback, where users directly report on their interest by adding entities to their own profile, or liking the activity with particular entity. Explicit feedback is not always available and sufficient, we also infer user preferences from the more abundant implicit feedback, which indirectly reflects opinion through observing user behavior, in this case, user's browsing history.


\end{document}
